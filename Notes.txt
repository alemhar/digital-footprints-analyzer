# Building a Personal Digital Footprint Analyzer with crewAI and Gradio

This document provides a complete guide to building a "Personal Digital Footprint Analyzer." This ethical, privacy-focused tool allows users to search for their own name online, receive a report on publicly available information, and get actionable advice on how to manage or remove it.

**Technology Stack:**
* **Python:** The core programming language.
* **crewAI:** A framework for orchestrating autonomous AI agents.
* **Gradio:** A library for creating simple, fast web interfaces for machine learning models.
* **APIs:** A search API (e.g., Serper) and an LLM API (e.g., OpenAI).

---

## 1. Important Ethical Considerations

This project's success and legitimacy depend on a strong ethical foundation.

### **Purpose: Empowerment, Not Investigation**
This tool is designed for **personal use only**. Its goal is to help individuals understand and manage their own digital footprint. It should never be marketed or used as a tool to investigate other people.

### **Public vs. Non-Public Data**
* **Public Data (OSINT):** The tool will only access Open Source Intelligence—information that is publicly available through search engines, public social media profiles, news articles, etc. This is generally permissible.
* **Non-Public Data:** The tool **must not** attempt to access information behind logins, paywalls, or in private databases. Doing so is illegal and a violation of privacy.

### **Inability to Auto-Remove Data**
The application **cannot** automatically remove data from the internet. It does not have access to the servers where the data is hosted. Its primary value is to **discover** information and **provide guidance** on how the user can request its removal themselves. This must be clearly communicated to the user.

---

## 2. Project Architecture with crewAI

We will create a "Privacy Audit Crew" composed of three specialized AI agents that work together sequentially to process a user's request.

### The Agents

1.  **`Public_Data_Finder_Agent`**
    * **Goal:** To find all public mentions, profiles, and data points associated with the user's name.
    * **Backstory:** A specialized OSINT tool that is fine-tuned to find traces of personal information across the web, from social networks to data aggregator sites.

2.  **`Data_Risk_Classifier_Agent`**
    * **Goal:** To analyze each piece of found information and classify its potential privacy risk (e.g., Low, Medium, High).
    * **Backstory:** A privacy expert who understands what constitutes Personally Identifiable Information (PII) and can assess the risk of its public exposure.

3.  **`Removal_Advisor_Agent`**
    * **Goal:** To generate clear, step-by-step instructions for how the user can attempt to remove each piece of high-risk information found.
    * **Backstory:** An expert in data privacy and removal requests who knows the standard procedures for contacting data brokers, using Google's content removal tools, and managing social media privacy settings.

### The Tasks

The agents will perform a sequence of tasks to generate the final report:

1.  **`find_data_task`**: The `Finder_Agent` searches for the user's name.
2.  **`classify_risk_task`**: The `Classifier_Agent` analyzes the search results and assigns a risk level to each finding.
3.  **`generate_report_task`**: The `Advisor_Agent` takes the classified findings and creates a comprehensive report with actionable removal steps.

---

## 3. Setup and Installation

### Prerequisites
* Python 3.8 or higher.
* Git (for cloning the repository).

### Steps

1.  **Clone the Repository (or create a project folder)**
    ```bash
    # Example command
    git clone [https://your-repo-url.git](https://your-repo-url.git)
    cd your-project-folder
    ```

2.  **Install Required Libraries**
    Create a `requirements.txt` file with the following content:
    ```
    gradio
    crewai
    crewai-tools
    python-dotenv
    openai
    serper-api-client
    ```
    Then, install them using pip:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Set Up API Keys**
    You will need API keys from a search provider and an LLM provider.
    * **Search:** [Serper.dev](https://serper.dev) (offers a free tier)
    * **LLM:** [OpenAI](https://platform.openai.com/api-keys)

    Create a file named `.env` in your project's root directory and add your keys to it. **Never commit this file to Git.**

    **.env file:**
    ```
    SERPER_API_KEY="your_serper_api_key_here"
    OPENAI_API_KEY="your_openai_api_key_here"
    ```

---

## 4. Full Python Code (`app.py`)

Create a file named `app.py` and paste the following code into it.

```python
import os
import gradio as gr
from crewai import Agent, Task, Crew, Process
from crewai_tools import SerperDevTool
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Check for API Keys ---
serper_api_key = os.getenv("SERPER_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

if not serper_api_key or not openai_api_key:
    raise ValueError("SERPER_API_KEY and OPENAI_API_KEY must be set in the .env file.")

# --- Define Tools ---
search_tool = SerperDevTool()

# --- 1. Define Your Privacy-Focused Agents ---
finder_agent = Agent(
    role='Personal Digital Footprint Finder',
    goal='Find public data, social media profiles, and mentions associated with a person\'s name.',
    backstory='You are an OSINT expert specialized in mapping the public digital footprint of individuals for privacy audit purposes.',
    verbose=True,
    tools=[search_tool]
)

risk_classifier_agent = Agent(
    role='Data Privacy Risk Classifier',
    goal='Analyze found data and classify it into High, Medium, or Low privacy risk categories.',
    backstory='As a privacy analyst, you identify Personally Identifiable Information (PII) and assess the risk of its public exposure.',
    verbose=True
)

removal_advisor_agent = Agent(
    role='Privacy Removal Advisor',
    goal='Create a step-by-step report on how to remove or mitigate identified privacy risks.',
    backstory='You are an expert in data removal processes. You provide clear, actionable advice for contacting data brokers, adjusting social media settings, and using search engine removal tools. You never promise removal, only guide the process.',
    verbose=True
)

# --- 2. Define the Privacy Audit Tasks ---
def create_privacy_tasks(full_name):
    find_data_task = Task(
        description=f"Conduct a thorough search for the full name: '{full_name}'. Look for social media, professional sites, data broker lists, and public forum mentions.",
        expected_output=f"A list of URLs and text snippets where the name '{full_name}' is mentioned.",
        agent=finder_agent
    )

    classify_risk_task = Task(
        description="For each piece of information found, classify its privacy risk. PII like home addresses, phone numbers, and private emails are HIGH risk. Public professional profiles are LOW risk. Old, non-PII forum posts are MEDIUM risk.",
        expected_output="A structured list of findings, each with a URL and a risk classification (High, Medium, Low).",
        agent=risk_classifier_agent,
        context=[find_data_task] # This task depends on the output of the find_data_task
    )
    
    generate_report_task = Task(
        description="Compile a final privacy report based on the classified findings. For each HIGH and MEDIUM risk item, provide actionable steps the user can take to request removal or enhance privacy. For a data broker, provide a general link to their opt-out page. For Google, mention their 'Remove outdated content' tool. The report should be in markdown format, easy to read, and empowering for the user.",
        expected_output=f"A comprehensive, well-formatted markdown report titled 'Your Personal Digital Footprint Report for {full_name}'. It should list each finding, its risk level, and clear, actionable steps for remediation.",
        agent=removal_advisor_agent,
        context=[classify_risk_task] # This task depends on the classified findings
    )
    return [find_data_task, classify_risk_task, generate_report_task]

# --- 3. Gradio Interface Function ---
def run_privacy_audit(full_name):
    if not full_name or len(full_name.split()) < 2:
        return "Please enter a full name to begin your privacy audit."
    
    tasks = create_privacy_tasks(full_name)
    
    privacy_crew = Crew(
        agents=[finder_agent, risk_classifier_agent, removal_advisor_agent],
        tasks=tasks,
        process=Process.sequential,
        verbose=2
    )
    
    result = privacy_crew.kickoff()
    return result

# --- 4. Launch the Gradio App ---
disclaimer = """
**Disclaimer:** This tool is for personal, educational purposes to help you understand your digital footprint.
It searches only publicly available information. The results may not be 100% accurate or complete.
This tool **cannot** remove data for you, but it provides guidance on how you can request removal from third-party sites.
Use this tool responsibly and only to search for your own name.
"""

iface = gr.Interface(
    fn=run_privacy_audit,
    inputs=gr.Textbox(lines=1, placeholder="Enter Your Full Name to Audit..."),
    outputs=gr.Markdown(label="Your Personal Digital Footprint Report"),
    title="Personal Digital Footprint Analyzer",
    description="Curious about what's publicly available about you online? Enter your name to generate a privacy report and get actionable advice on how to manage your digital identity.",
    article=disclaimer,
    examples=[["Jane Doe"], ["John Smith"]]
)

if __name__ == "__main__":
    iface.launch()

```

---

## 5. How to Run the Application

1.  Open your terminal or command prompt.
2.  Navigate to your project directory.
3.  Run the following command:

    ```bash
    python app.py
    ```

4.  The terminal will output a local URL, usually `http://127.0.0.1:7860`.
5.  Open this URL in your web browser to use the application.

---

## 6. Positioning on LinkedIn (Example Post)

When you are ready to share your project, use a responsible and engaging tone.

> I built a free AI-powered tool to help you audit your own digital footprint! 🕵️‍♂️
>
> In today's world, our data is everywhere. I wanted to see if I could use `crewAI` and `Gradio` to create a simple app that not only finds your publicly available information but also gives you actionable steps to manage it.
>
> Enter your name, and the AI crew will:
> 1.  🔎 **Search** for public mentions.
> 2.  📊 **Classify** potential privacy risks.
> 3.  ✍️ **Generate a report** with guidance on how you can request data removal from sites.
>
> This is an educational project, and I'd love your feedback! It's a demonstration of how AI can be used for personal empowerment and privacy.
>
> You can try it here: [Link to your Gradio App - e.g., on Hugging Face Spaces]
>
> `#Python #AI #CyberSecurity #Privacy #OSINT #Gradio #crewAI #DigitalFootprint #DataPrivacy`